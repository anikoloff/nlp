{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction: 20 Newsgroups Data Set\n",
    "\n",
    "11.08.19\n",
    "\n",
    "(Data set: http://qwone.com/~jason/20Newsgroups/)\n",
    "\n",
    "Take the 20 newsgroups dataset and use different methods of topic modeling. The goal is to determine which method, if any, best reproduces the topics represented by the newsgroups. \n",
    "\n",
    "Methods:\n",
    "- Latent Semantic Analysis (LSA)\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Non-Negative Matrix Factorization (NNMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                            remove=('headers', 'footers', 'quotes'))\n",
    "news = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data.\n",
    "# Create the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer(max_df=0.95,\n",
    "                             min_df=2,\n",
    "                             max_features=1000,\n",
    "                             stop_words='english')\n",
    "news_tfidf=vectorizer.fit_transform(news)\n",
    "\n",
    "# LDA can only use raw term counts for LDA\n",
    "# because it is a probabilistic graphical model.\n",
    "vectorizer2 = CountVectorizer(max_df=0.95,\n",
    "                             min_df=2,\n",
    "                             max_features=1000,\n",
    "                             stop_words='english')\n",
    "news_tf=vectorizer2.fit_transform(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=20\n",
    "\n",
    "# Link words to topics.\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extract the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=[x for x in chosenlist]\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "A process of applying PCA to a tf-idf matrix: we reduce the tf-idf-weighted term-document matrix into a lower-dimensional space to get clusters of terms that should reflect a topic. Shortcomings: the link between words and topics is not very clear: some words may have high negative loadings on a component, etc.\n",
    "\n",
    "### Probabilistic LSA\n",
    "(Note: it is not supported by scikit-learn)\n",
    "\n",
    "pLSA (=pLSI, Probabilistic Latent Semantic Indexing) assumes the existence of set of topics, that set being unknown at the start. Opposite of LSA where we start with the data and solve for a set of component-topics.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA) \n",
    "is a Bayesian implementation of pLSA; it includes: \n",
    "- sparse Dirichlet priors for estimating the probability that a topic will be in a document, \n",
    "- the probability that a word will be in a topic.\n",
    "\n",
    "### Topic modeling with Non-Negative Matrix Factorization\n",
    "- like PCA, searches for 2 matrices that result in the tf-idf matrix,\n",
    "- unlike PCA, we apply constraint that all three matrices must contain no negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA.\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "news_lsa = lsa.fit_transform(news_tfidf)\n",
    "\n",
    "components_lsa = word_topic(news_tfidf, news_lsa, terms)\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA.\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning.\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever).\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol.\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached.\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating.\n",
    "          random_state=0\n",
    "         )\n",
    "news_lda = lda.fit_transform(news_tf) \n",
    "\n",
    "components_lda = word_topic(news_tf, news_lda, terms)\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF.\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # How starting value are calculated.\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1).\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever).\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve.\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol.\n",
    "          verbose=0 # amount of output to give while iterating.\n",
    "         )\n",
    "news_nmf = nmf.fit_transform(news_tfidf) \n",
    "\n",
    "components_nmf = word_topic(news_tfidf, news_nmf, terms)\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "             LSA             LDA        NNMF\n",
      "0     don 161.96    space 541.44   good 5.92\n",
      "0    just 161.61       00 456.26     ve 5.12\n",
      "0    like 160.79     nasa 187.38   time 4.96\n",
      "0     know 150.2      new 180.46   just 3.97\n",
      "0  people 144.89       10 173.54    like 3.6\n",
      "0   think 134.99   launch 164.58    don 3.57\n",
      "0    time 118.36     1993 160.14    did 3.26\n",
      "0     does 115.0       20 155.88    got 3.26\n",
      "0     use 114.24  program 141.86   know 3.23\n",
      "0    good 113.05     data 139.16  think 3.07\n",
      "Topic 1:\n",
      "              LSA           LDA          NNMF\n",
      "1    thanks 64.82    use 299.92     card 8.24\n",
      "1   windows 57.77   scsi 299.23    video 4.09\n",
      "1        use 38.3    like 258.5  monitor 2.69\n",
      "1      mail 34.52   just 242.63  drivers 2.17\n",
      "1       card 34.1    don 203.27    cards 2.11\n",
      "1      file 33.22  power 201.21      bus 2.04\n",
      "1  software 30.19   time 200.03   driver 1.87\n",
      "1       dos 29.23     car 192.6      vga 1.85\n",
      "1   program 29.22  speed 189.58  windows 1.78\n",
      "1     drive 29.09   good 180.76   memory 1.71\n",
      "Topic 2:\n",
      "                LSA             LDA             NNMF\n",
      "2         god 80.28     file 644.25         god 16.9\n",
      "2        does 32.74  program 391.22       jesus 5.94\n",
      "2       jesus 31.33      use 365.62     believe 3.67\n",
      "2      thanks 27.02     entry 346.9       bible 3.56\n",
      "2      windows 20.9   output 331.42      people 3.17\n",
      "2       bible 19.88   window 323.39      christ 3.01\n",
      "2  christians 17.86    files 204.82   christian 2.87\n",
      "2   christian 17.48    using 189.17       faith 2.81\n",
      "2     believe 17.26  windows 186.44  christians 2.71\n",
      "2        know 16.58     does 177.75         say 2.69\n",
      "Topic 3:\n",
      "                LSA                LDA          NNMF\n",
      "3         use 36.68         key 523.26     game 6.45\n",
      "3         key 34.52  encryption 349.41     team 5.36\n",
      "3      people 31.59        chip 248.31     year 4.21\n",
      "3  government 31.07         use 245.04    games 3.61\n",
      "3        chip 20.41  government 219.08   season 2.76\n",
      "3  encryption 18.82     clipper 208.76  players 2.71\n",
      "3     clipper 17.15        keys 195.91      play 2.5\n",
      "3       using 16.31      public 172.05   hockey 2.29\n",
      "3         law 16.16    security 171.28      win 2.09\n",
      "3      public 15.07         law 161.62    think 1.82\n",
      "Topic 4:\n",
      "             LSA               LDA            NNMF\n",
      "4     know 54.48      drive 351.36         new 6.9\n",
      "4      don 41.65        disk 187.8         00 4.91\n",
      "4     just 33.11     drives 151.38       sale 3.93\n",
      "4     does 28.66       hard 145.85         10 3.47\n",
      "4    thanks 27.6        use 131.77      price 2.81\n",
      "4     like 23.25   internet 106.05      offer 2.63\n",
      "4    think 16.49  controller 99.09   shipping 2.35\n",
      "4  anybody 11.98   anonymous 86.08  condition 2.33\n",
      "4  advance 10.18  information 81.3         20 2.18\n",
      "4        ve 9.57        data 80.09       like 2.17\n",
      "Topic 5:\n",
      "                 LSA            LDA              NNMF\n",
      "5          edu 55.55    just 687.71      thanks 13.39\n",
      "5        thanks 36.3    know 588.99         mail 5.34\n",
      "5         mail 26.68     don 580.94      advance 4.88\n",
      "5         know 23.85    like 578.77         know 4.36\n",
      "5  information 18.73  people 547.16       looking 3.6\n",
      "5        email 16.77    said 476.03           hi 3.59\n",
      "5      address 15.25    time 470.92         info 2.96\n",
      "5         send 15.07    didn 413.75         help 2.95\n",
      "5         does 14.98   think 398.34         does 2.93\n",
      "5          com 14.49     did 363.82  information 2.72\n",
      "Topic 6:\n",
      "            LSA             LDA           NNMF\n",
      "6   drive 40.71      game 368.2  windows 12.63\n",
      "6   thanks 19.4     team 366.73      file 7.65\n",
      "6    know 19.21      year 323.0        use 6.6\n",
      "6     scsi 17.6    games 251.45      files 5.6\n",
      "6     car 17.56   season 242.13       dos 5.45\n",
      "6     edu 13.82     play 215.05    window 5.15\n",
      "6   people 12.9     good 199.39   program 4.95\n",
      "6    hard 12.84  players 179.96     using 4.47\n",
      "6  drives 12.31   hockey 177.42   problem 4.39\n",
      "6    price 12.1    think 171.12    thanks 3.21\n",
      "Topic 7:\n",
      "             LSA            LDA             NNMF\n",
      "7      edu 62.09     new 193.49        edu 14.39\n",
      "7     just 42.14  people 162.76        soon 3.08\n",
      "7      don 20.06    years 155.6  university 2.12\n",
      "7      soon 14.7    time 150.91          cs 1.96\n",
      "7    think 13.46    like 149.36       email 1.61\n",
      "7  windows 13.19     use 128.47        mail 1.59\n",
      "7      file 9.94  health 127.59        like 1.46\n",
      "7         ve 9.6    just 120.44        just 1.46\n",
      "7       try 9.22     don 118.07         don 1.45\n",
      "7    window 8.87     000 116.02          ftp 1.4\n",
      "Topic 8:\n",
      "             LSA            LDA             NNMF\n",
      "8      key 37.05    don 1044.38        key 10.91\n",
      "8     just 26.85  people 1041.1        chip 6.25\n",
      "8      god 22.34   think 878.14         use 4.94\n",
      "8     chip 21.79    just 838.29   encryption 4.6\n",
      "8      use 16.46    like 814.32     clipper 4.42\n",
      "8      edu 15.78    know 670.06        keys 4.15\n",
      "8      like 15.5    does 540.83  government 3.64\n",
      "8     keys 14.37    good 499.53      public 2.69\n",
      "8  clipper 14.35    make 469.41      escrow 2.62\n",
      "8     good 13.57      say 469.0         bit 2.48\n",
      "Topic 9:\n",
      "            LSA         LDA             NNMF\n",
      "9     car 43.35  edu 362.45      drive 13.45\n",
      "9    like 29.14   cx 306.34         scsi 5.0\n",
      "9    bike 22.27  com 260.04        hard 3.84\n",
      "9     new 22.08   w7 246.86        disk 3.66\n",
      "9    good 16.33  chz 159.25      drives 3.62\n",
      "9  thanks 14.87   lk 151.99         ide 2.63\n",
      "9    just 14.37   17 140.18  controller 2.51\n",
      "9  looking 13.6   02 126.99      floppy 2.38\n",
      "9  window 12.53    ah 117.6         mac 1.71\n",
      "9   engine 11.7   03 113.72          cd 1.67\n",
      "Topic 10:\n",
      "             LSA               LDA         NNMF\n",
      "10    just 73.15        edu 640.95   just 18.11\n",
      "10     did 13.91  available 478.84     don 4.18\n",
      "10    game 13.53         ftp 392.3    like 3.92\n",
      "10  thanks 13.15      image 345.37  people 3.26\n",
      "10  israel 12.35       file 343.21   think 3.13\n",
      "10      got 9.57        use 330.31    know 3.04\n",
      "10  thought 8.82    version 322.12    good 2.34\n",
      "10     jews 8.59        pub 320.51    does 2.25\n",
      "10  israeli 7.87   graphics 286.07     say 2.18\n",
      "10    jewish 7.7   software 281.04    time 2.13\n",
      "Topic 11:\n",
      "              LSA                    LDA          NNMF\n",
      "11     does 49.78              mr 379.92    does 15.39\n",
      "11     card 34.14  stephanopoulos 313.06     know 7.94\n",
      "11     video 18.3        president 306.9     like 2.93\n",
      "11       edu 18.2            think 212.0   thanks 2.79\n",
      "11     know 18.14            know 201.19      just 2.6\n",
      "11      car 13.59           going 196.68  anybody 2.44\n",
      "11    israel 13.0             don 185.09   people 2.12\n",
      "11  drivers 12.89            said 137.91      don 2.04\n",
      "11  monitor 11.58          people 136.11    think 1.92\n",
      "11      new 10.81              ms 114.81      use 1.91\n",
      "Topic 12:\n",
      "               LSA               LDA             NNMF\n",
      "12       don 32.98       god 1233.07     people 14.51\n",
      "12      card 23.65      jesus 590.02         don 4.06\n",
      "12     think 22.59     people 484.17  government 3.95\n",
      "12        00 20.77       does 395.97       think 3.63\n",
      "12      just 18.54    believe 352.05        just 3.56\n",
      "12      sale 18.06      bible 335.81        right 3.2\n",
      "12       new 14.39         say 301.0        like 3.18\n",
      "12    people 14.16      think 295.52        know 2.89\n",
      "12  shipping 11.91  christian 285.81        said 2.48\n",
      "12      mail 11.58        know 280.1         say 2.46\n",
      "Topic 13:\n",
      "              LSA          LDA         NNMF\n",
      "13      like 47.6  ax 62264.11   like 19.79\n",
      "13    space 28.53  max 4470.47    just 4.87\n",
      "13     know 18.03   g9v 1153.3     don 4.42\n",
      "13      don 16.78  b8f 1069.29     know 4.1\n",
      "13      new 16.73   a86 895.88  people 3.51\n",
      "13      use 15.34    pl 713.11   think 3.51\n",
      "13  program 12.45   145 692.69     use 3.29\n",
      "13     does 11.28   1d9 637.52     does 3.1\n",
      "13     nasa 10.82    0t 471.97  sounds 2.84\n",
      "13     list 10.32    1t 467.35    look 2.82\n",
      "Topic 14:\n",
      "             LSA            LDA         NNMF\n",
      "14    like 71.79      bit 223.9    don 16.67\n",
      "14  people 12.56     key 147.08    know 9.93\n",
      "14   think 11.49    chip 146.87    just 4.86\n",
      "14     edu 11.17  number 103.91    think 4.7\n",
      "14  thanks 11.15      use 92.03  people 4.31\n",
      "14    sounds 9.9     scsi 84.96    like 4.31\n",
      "14     card 9.16       32 79.56    want 3.55\n",
      "14    looks 7.66     data 76.94    good 2.74\n",
      "14     chip 7.47   serial 76.19     use 2.63\n",
      "14       hi 7.14     like 72.45     say 2.52\n",
      "Topic 15:\n",
      "              LSA              LDA         NNMF\n",
      "15  windows 24.57   windows 458.16    car 13.42\n",
      "15      car 23.79      card 366.13    cars 2.94\n",
      "15     file 22.68        dos 289.6  engine 1.88\n",
      "15    files 18.68       use 287.45    like 1.78\n",
      "15      new 17.44      like 252.57   speed 1.57\n",
      "15    drive 15.92      does 240.34    good 1.54\n",
      "15   people 14.57      know 238.23    just 1.52\n",
      "15      dos 11.96   problem 215.37     don 1.31\n",
      "15       00 11.44    thanks 210.05      new 1.3\n",
      "15     just 10.87  software 203.97    know 1.22\n",
      "Topic 16:\n",
      "             LSA        LDA         NNMF\n",
      "16   think 51.41  10 486.47  think 15.71\n",
      "16     does 33.2  db 438.21     don 4.65\n",
      "16    good 31.18  25 390.32  people 3.63\n",
      "16    just 13.96   55 347.2    just 3.33\n",
      "16     book 11.4  16 322.01    like 3.02\n",
      "16    space 9.91  11 320.09    know 2.44\n",
      "16     true 8.27  12 304.34    good 2.24\n",
      "16    point 8.22  15 300.79     say 2.12\n",
      "16      say 8.17  14 287.33  really 2.09\n",
      "16  looking 7.66  20 268.89     way 1.96\n",
      "Topic 17:\n",
      "             LSA            LDA          NNMF\n",
      "17  people 41.71  thanks 343.66  israel 11.47\n",
      "17      ve 34.71    know 326.42  israeli 6.16\n",
      "17    know 18.77    like 310.96     jews 5.42\n",
      "17     got 15.86    mail 297.14   jewish 3.22\n",
      "17   heard 14.91     edu 258.78    state 2.59\n",
      "17   space 13.42    good 243.76      war 2.46\n",
      "17      ll 13.41     new 243.36      did 2.36\n",
      "17    good 10.72    does 226.87    peace 2.17\n",
      "17     seen 9.44    just 215.55    people 2.1\n",
      "17     file 8.68     don 206.03     said 1.94\n",
      "Topic 18:\n",
      "            LSA                LDA          NNMF\n",
      "18   good 48.73     armenian 421.3   space 10.42\n",
      "18     use 19.5      people 387.97     nasa 3.84\n",
      "18    time 16.5      turkish 364.5      data 2.1\n",
      "18   want 12.58   armenians 309.27  program 2.08\n",
      "18   sale 11.83        jews 260.97   shuttle 2.0\n",
      "18   make 11.02      israel 228.39   launch 1.95\n",
      "18  offer 10.31        said 223.27     like 1.73\n",
      "18    like 9.69      turkey 172.49    orbit 1.71\n",
      "18      00 9.25  government 160.93     earth 1.7\n",
      "18     don 8.89     israeli 153.58  station 1.58\n",
      "Topic 19:\n",
      "              LSA               LDA          NNMF\n",
      "19      com 52.26     people 469.66     com 13.48\n",
      "19  article 15.23        gun 335.35     list 2.55\n",
      "19      car 13.53  government 290.9     mail 1.69\n",
      "19     like 13.41      right 248.95      don 1.59\n",
      "19     list 12.32        law 234.01  article 1.57\n",
      "19   deleted 11.8      state 222.73    email 1.54\n",
      "19  looking 11.44        don 214.64      edu 1.48\n",
      "19       ve 11.17      think 180.46     like 1.45\n",
      "19       gun 9.83       like 175.46     dave 1.42\n",
      "19  software 9.54       just 173.53  address 1.39\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth (20 news groups):\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x\t\n",
    "- rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey\t\n",
    "- sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- misc.forsale\t\n",
    "- talk.politics.misc\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast\t\n",
    "- talk.religion.misc\n",
    "- alt.atheism\n",
    "- soc.religion.christian\n",
    "\n",
    "\n",
    "### Summary\n",
    "In the results above, some topics are shared, though the order of topics varies. Additionally, the content of some of the topics varies considerably across methods. It's best to use multiple methods when exploring topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
