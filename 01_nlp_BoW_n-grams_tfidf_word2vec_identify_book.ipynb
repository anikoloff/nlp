{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised NLP: Identify Book\n",
    "\n",
    "10.13.19\n",
    "\n",
    "Parse and explore data. Use different language modeling techniques to predict whether a sentence comes from \"Alice's Adventures in Wonderland\" by Lewis Carroll or \"Persuasion\" by Jane Austen.\n",
    "\n",
    "Topics:\n",
    "- Text Preprocessing and Exploration (spaCy, NLTK):\n",
    "    - data cleaning, \n",
    "    - tokens, \n",
    "    - stop words, \n",
    "    - lemmas, \n",
    "    - sentences,\n",
    "    - named entities;\n",
    "- Feature Engineering/Language Modeling:\n",
    "    - Bag of Words (CountVectorizer), \n",
    "    - N-grams,\n",
    "    - tf-idf (TfidfVectorizer),\n",
    "    - word2vec (Gensim);\n",
    "- Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import gensim \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('gutenberg')\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Print the first 100 characters of Alice.\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Some ways to clean text:\n",
    "- correcting typos and misspelled words,\n",
    "- dealing with abbreviations,\n",
    "- lowercasing/uppercasing,\n",
    "- removing emojis,\n",
    "- removing stopwords,\n",
    "- normalizing the words (lemmatization, stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "# Remove title.\n",
    "# Match all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "# Remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "# Remove \"new line\" characters and other types of extra whitespaces.\n",
    "# Split and rejoin sentences.\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "I'll use spaCy to parse the novels into tokens. When one calls spaCy on text, it automatically parses the text, tokenizing the string by breaking it into words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34408 tokens long\n",
      "The first three tokens are \"Alice was beginning\"\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Explore the objects.\n",
    "print('The alice_doc object is a {} object.'.format(type(alice_doc)))\n",
    "print('It is {} tokens long'.format(len(alice_doc)))\n",
    "print('The first three tokens are \"{}\"'.format(alice_doc[:3]))\n",
    "print('The type of each token is {}'.format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('the', 1524), ('and', 796), ('to', 724), ('a', 611), ('I', 533), ('it', 524), ('she', 508), ('of', 499), ('said', 453), ('Alice', 394)]\n",
      "Persuasion: [('the', 3120), ('to', 2775), ('and', 2738), ('of', 2563), ('a', 1529), ('in', 1346), ('was', 1329), ('had', 1177), ('her', 1159), ('I', 1118)]\n"
     ]
    }
   ],
   "source": [
    "# Review the frequency of all tokens including stop words.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Here is a list of the stopwords identified by NLTK.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('said', 453), ('Alice', 394), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('King', 61)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 254), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166)]\n"
     ]
    }
   ],
   "source": [
    "# Review the frequency of tokens without stopwords.\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('say', 477), ('Alice', 394), ('think', 131), ('go', 130), ('little', 126), ('look', 106), ('know', 103), ('come', 96), ('like', 92), ('begin', 91)]\n",
      "Persuasion: [('Anne', 496), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('think', 257), ('Mr', 254), ('know', 252), ('good', 225), ('Wentworth', 217), ('Lady', 191)]\n"
     ]
    }
   ],
   "source": [
    "# Review the frequency of lemmas without stop words.\n",
    "def lemma_frequencies(text, include_stop=False):\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    return Counter(lemmas)\n",
    "\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'begin', 'go', 'look', 'Alice', 'like', 'little', 'come', 'say'}\n",
      "Unique to Persuasion: {'Elliot', 'Captain', 'Wentworth', 'Mrs', 'Lady', 'Anne', 'Mr', 'good'}\n"
     ]
    }
   ],
   "source": [
    "# Identify lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-level Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1860 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(alice_doc.sents)\n",
    "print('Alice in Wonderland has {} sentences.'.format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print('Here is an example: \\n{}\\n'.format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 words in this sentence, and 25 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence.\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print(('There are {} words in this sentence, and {} of them are'\n",
    "       ' unique.').format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Alice\n",
      "PERSON Alice\n",
      "ORG White Rabbit\n",
      "ORG VERY\n",
      "PERSON Alice\n",
      "GPE Rabbit\n",
      "GPE Rabbit\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "PERSON Alice\n"
     ]
    }
   ],
   "source": [
    "# Extract the first ten entities.\n",
    "entities = list(alice_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bill', 'the Knave of Hearts', 'King', 'Ma', 'Latitude', 'Curiouser', 'Dodo', 'Hatter', 'Mary Ann', 'Serpent', 'Miss', 'began:--', 'Brandy', 'Somebody', 'sadly:--', 'Game', 'Soo', 'the Queen of Hearts', 'The Knave of Hearts', 'Chorus', 'Said', 'Boots', 'Shark', 'Edwin', 'Knave', 'Fifteenth', 'Soles', 'Beau', 'Fury', 'Herald', \"W. RABBIT'\", 'Jack', 'WILLIAM', 'Stupid', 'Stuff', 'Dinah', 'Queen', 'William the Conqueror', 'Duck', 'Queens', 'Duchess', 'Morcar', 'Mercia', 'ALICE', 'Hjckrrh', 'Cheshire Puss', 'Hare', 'Kings', 'Gryphon', 'Longitude', 'Pat', 'Turtle', 'yer honour', 'words:--', 'Edgar Atheling', 'Dinn', 'Tut', 'Idiot', 'indeed:--', 'William', 'Lobster', 'Lizard', 'Tillie', 'Beautiful', 'Magpie', 'Footman', 'Down', 'Ou est ma chatte', 'Mabel', 'Tortoise', 'Shakespeare', 'follows:--', 'Alice', 'Lobster Quadrille', 'ye', 'Ada'}\n"
     ]
    }
   ],
   "source": [
    "# All of the unique entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Walter Elliot\n",
      "ORG Kellynch Hall\n",
      "GPE Somersetshire\n",
      "TIME an idle hour\n",
      "DATE the last century\n",
      "WORK_OF_ART ELLIOT OF KELLYNCH HALL\n",
      "PERSON Walter Elliot\n",
      "DATE March 1 , 1760\n",
      "DATE July 15 , 1784\n",
      "PERSON Elizabeth\n"
     ]
    }
   ],
   "source": [
    "# Extract the first ten entities.\n",
    "entities = list(persuasion_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cousin Charles', 'the Mr Wentworth of Monkford', 'Hayters', \"Mrs Musgrove's\", 'Captain Benwick', 'Master Harry', 'Belmont', 'Louise', 'Hayter', 'Carteret', 'God', 'an Anne Elliot', 'Shabby', 'a Louisa Musgrove', 'Mrs Clay', 'Louisa', 'Gibraltar', 'Basil Morley', 'William Walter Elliot', 'Mr--', 'Benwick', 'Byron', 'Walter Elliot', 'Sophia', 'Musgroves', \"Walter Elliot's\", 'Charles Smith', 'Elliot', 'Mr Wentworth', 'Lady Alicia', 'Bath', \"Lady Russell's\", \"Captain Wentworth's\", \"Louisa Musgrove's\", 'Henrietta', 'Lady Mary Maclean', \"Mrs Charles's\", 'Captain Harville', 'Thornberry', 'Mrs Speed', 'Mr Shepherd', 'Brigden', 'Sophy', 'Anne Elliot', \"Mrs Croft's\", 'Hamilton', 'Heir', 'Edward', 'Michaelmas', 'John Shepherd', 'Tunbridge Wells', \"Lady Dalrymple's\", \"Elizabeth Elliot's\", 'Dick', 'a Mrs Wallis', 'Henry', 'Winthrop', 'Mrs Croft', 'Archibald Drew', 'Mark', 'Mrs Musgrove', 'Louisa Musgrove', 'Cobb', 'F. W.', \"Frederick Wentworth's\", 'Kellynch Lodge', 'Mr Shepherd one morning', 'William', 'Crofts', 'Mr Elliot', 'Anne haggard', \"Mrs Clay's\", 'James', 'Robinson', 'Viscountess Dalrymple', 'Bahama', \"Mrs Smith's\", 'Mrs Rooke', 'James Benwick', 'Harville', 'Lady Wentworth', 'Shirley', \"Mrs Wallis's\", 'Musgrove', 'Mrs Wallis', 'nay', 'Marlborough Buildings', 'James Stevenson', 'Dick Musgrove', 'Lady Russell', 'Archibald', 'Hall', 'pleased;-- Charles Hayter', 'Mr Smith', 'Charles Musgrove', 'The Mr Musgroves', 'Elegance', 'Baldwin', 'Fanny Harville', 'Mrs Shirley', 'Croft', 'the Captain Wentworth', 'Frederick', 'Wallis', \"Captain Benwick's\", 'Pinny', 'the Miss Musgroves', \"Henry Russell's\", 'Lady Elliot', \"Mrs Charles Musgrove's\", 'Anne felt', \"Lady Elliot's\", 'Mrs Hayter', 'Wentworth', \"Mrs Harville's\", 'Laura Place', 'Poor Harville', 'Basil', 'Captain Wentworth', \"Mr Robinson's\", 'Mr Musgrove', 'Nay', 'Monkford', 'Wallises', 'Lady Dalrymple', 'Mr Robinson', 'Mary coarse', 'Walter', 'Brand', 'Charles', 'Lord', 'this Mr Musgrove', 'Shepherd', 'Frederick Wentworth', 'Mrs Harville', 'Westgate Buildings', 'Larolles', 'Charles II', 'Grappler', 'Elizabeth', 'Scott', 'Sarah', 'I. No', 'Giaour', 'Emma', 'Anne', 'Mrs Charles Musgrove', 'Miss Atkinson', 'Captain Wentworth long', 'Pooles', 'Richard', 'Asp', 'Lady Mary Grierson', 'Elliot marry', 'Mrs Smith', 'Kellynch', 'Kellynch Hall', 'Charles Hayter', \"Lady Wentworth'\", 'Mary', 'Lyme', 'Esq', \"Charles Hayter's\", 'Mrs Charles', 'Shropshire'}\n"
     ]
    }
   ],
   "source": [
    "# All of the unique entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(persuasion_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, 'Carroll'] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, 'Austen'] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation, lemmatize the tokens.\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll\n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll\n",
       "2     remarkable Alice think way hear rabbit oh dear  Carroll\n",
       "3                                            oh dear  Carroll\n",
       "4                                         shall late  Carroll"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Evaluation\n",
    "\n",
    "# Bag of Words (BoW)\n",
    "For each observation, count the occurance of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709, 4953)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>1760</th>\n",
       "      <th>1784</th>\n",
       "      <th>1785</th>\n",
       "      <th>1787</th>\n",
       "      <th>1789</th>\n",
       "      <th>1791</th>\n",
       "      <th>1800</th>\n",
       "      <th>1803</th>\n",
       "      <th>...</th>\n",
       "      <th>younker</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4953 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   15  16  1760  1784  1785  1787  1789  1791  1800  1803  ...  younker  \\\n",
       "0   0   0     0     0     0     0     0     0     0     0  ...        0   \n",
       "1   0   0     0     0     0     0     0     0     0     0  ...        0   \n",
       "2   0   0     0     0     0     0     0     0     0     0  ...        0   \n",
       "3   0   0     0     0     0     0     0     0     0     0  ...        0   \n",
       "4   0   0     0     0     0     0     0     0     0     0  ...        0   \n",
       "\n",
       "   youth  youthful  zeal  zealand  zealous  zealously  zigzag  \\\n",
       "0      0         0     0        0        0          0       0   \n",
       "1      0         0     0        0        0          0       0   \n",
       "2      0         0     0        0        0          0       0   \n",
       "3      0         0     0        0        0          0       0   \n",
       "4      0         0     0        0        0          0       0   \n",
       "\n",
       "                                                text   author  \n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2     remarkable Alice think way hear rabbit oh dear  Carroll  \n",
       "3                                            oh dear  Carroll  \n",
       "4                                         shall late  Carroll  \n",
       "\n",
       "[5 rows x 4953 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_bow = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "\n",
    "print(sentences_bow.shape)\n",
    "sentences_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_and_evaluation(data):\n",
    "    Y = data['author']\n",
    "    X = np.array(data.drop(['text','author'], 1))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    rfc = RandomForestClassifier()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    gbc.fit(X_train, y_train)\n",
    "\n",
    "    print('Logistic Regression Scores:')\n",
    "    print('Training set score:', lr.score(X_train, y_train))\n",
    "    print('Test set score:', lr.score(X_test, y_test))\n",
    "\n",
    "    print('\\nRandom Forest Scores:')\n",
    "    print('Training set score:', rfc.score(X_train, y_train))\n",
    "    print('Test set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "    print('\\nGradient Boosting Scores:')\n",
    "    print('Training set score:', gbc.score(X_train, y_train))\n",
    "    print('Test set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.9378102189781022\n",
      "Test set score: 0.8800350262697023\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9652554744525548\n",
      "Test set score: 0.8524518388791593\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.8402919708029197\n",
      "Test set score: 0.8288091068301225\n"
     ]
    }
   ],
   "source": [
    "# Scores on data with BoW features. \n",
    "model_and_evaluation(sentences_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "Words in context.\n",
    "\n",
    "**Use 2-grams (bigrams):**\n",
    "For each observation, count the occurance of each word couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709, 30608)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15 1784</th>\n",
       "      <th>16 1810</th>\n",
       "      <th>1760 married</th>\n",
       "      <th>1784 elizabeth</th>\n",
       "      <th>1785 anne</th>\n",
       "      <th>1787 bear</th>\n",
       "      <th>1789 mary</th>\n",
       "      <th>1803 dear</th>\n",
       "      <th>1806 have</th>\n",
       "      <th>1810 charles</th>\n",
       "      <th>...</th>\n",
       "      <th>zeal dwell</th>\n",
       "      <th>zeal sport</th>\n",
       "      <th>zeal think</th>\n",
       "      <th>zealand australia</th>\n",
       "      <th>zealous officer</th>\n",
       "      <th>zealous subject</th>\n",
       "      <th>zealously discharge</th>\n",
       "      <th>zigzag go</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   15 1784  16 1810  1760 married  1784 elizabeth  1785 anne  1787 bear  \\\n",
       "0        0        0             0               0          0          0   \n",
       "1        0        0             0               0          0          0   \n",
       "2        0        0             0               0          0          0   \n",
       "3        0        0             0               0          0          0   \n",
       "4        0        0             0               0          0          0   \n",
       "\n",
       "   1789 mary  1803 dear  1806 have  1810 charles  ...  zeal dwell  zeal sport  \\\n",
       "0          0          0          0             0  ...           0           0   \n",
       "1          0          0          0             0  ...           0           0   \n",
       "2          0          0          0             0  ...           0           0   \n",
       "3          0          0          0             0  ...           0           0   \n",
       "4          0          0          0             0  ...           0           0   \n",
       "\n",
       "   zeal think  zealand australia  zealous officer  zealous subject  \\\n",
       "0           0                  0                0                0   \n",
       "1           0                  0                0                0   \n",
       "2           0                  0                0                0   \n",
       "3           0                  0                0                0   \n",
       "4           0                  0                0                0   \n",
       "\n",
       "   zealously discharge  zigzag go  \\\n",
       "0                    0          0   \n",
       "1                    0          0   \n",
       "2                    0          0   \n",
       "3                    0          0   \n",
       "4                    0          0   \n",
       "\n",
       "                                                text   author  \n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2     remarkable Alice think way hear rabbit oh dear  Carroll  \n",
       "3                                            oh dear  Carroll  \n",
       "4                                         shall late  Carroll  \n",
       "\n",
       "[5 rows x 30608 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 2-grams: ngram_range parameter = (2,2).\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(sentences['text'])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_bigram = pd.concat([bow_df, sentences[['text', 'author']]], axis=1)\n",
    "print(sentences_bigram.shape)\n",
    "sentences_bigram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.9059854014598541\n",
      "Test set score: 0.782399299474606\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9445255474452555\n",
      "Test set score: 0.7968476357267951\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.7652554744525547\n",
      "Test set score: 0.7578809106830122\n"
     ]
    }
   ],
   "source": [
    "model_and_evaluation(sentences_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use both 1-gram and 2-gram features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709, 35559)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15</th>\n",
       "      <th>15 1784</th>\n",
       "      <th>16</th>\n",
       "      <th>16 1810</th>\n",
       "      <th>1760</th>\n",
       "      <th>1760 married</th>\n",
       "      <th>1784</th>\n",
       "      <th>1784 elizabeth</th>\n",
       "      <th>1785</th>\n",
       "      <th>1785 anne</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand australia</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealous officer</th>\n",
       "      <th>zealous subject</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zealously discharge</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zigzag go</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   15  15 1784  16  16 1810  1760  1760 married  1784  1784 elizabeth  1785  \\\n",
       "0   0        0   0        0     0             0     0               0     0   \n",
       "1   0        0   0        0     0             0     0               0     0   \n",
       "2   0        0   0        0     0             0     0               0     0   \n",
       "3   0        0   0        0     0             0     0               0     0   \n",
       "4   0        0   0        0     0             0     0               0     0   \n",
       "\n",
       "   1785 anne  ...  zealand australia  zealous  zealous officer  \\\n",
       "0          0  ...                  0        0                0   \n",
       "1          0  ...                  0        0                0   \n",
       "2          0  ...                  0        0                0   \n",
       "3          0  ...                  0        0                0   \n",
       "4          0  ...                  0        0                0   \n",
       "\n",
       "   zealous subject  zealously  zealously discharge  zigzag  zigzag go  \\\n",
       "0                0          0                    0       0          0   \n",
       "1                0          0                    0       0          0   \n",
       "2                0          0                    0       0          0   \n",
       "3                0          0                    0       0          0   \n",
       "4                0          0                    0       0          0   \n",
       "\n",
       "                                                text   author  \n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2     remarkable Alice think way hear rabbit oh dear  Carroll  \n",
       "3                                            oh dear  Carroll  \n",
       "4                                         shall late  Carroll  \n",
       "\n",
       "[5 rows x 35559 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use both 1-gram and 2-gram together: ngram_range=(1,2)\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_both = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "print(sentences_both.shape)\n",
    "sentences_both.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.9556204379562043\n",
      "Test set score: 0.8791593695271454\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9652554744525548\n",
      "Test set score: 0.8445709281961471\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.8402919708029197\n",
      "Test set score: 0.8279334500875657\n"
     ]
    }
   ],
   "source": [
    "model_and_evaluation(sentences_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Take into account the meanings of the words as well as their number of occurrences. \n",
    "\n",
    "Parameters:\n",
    "- max_df=0.5: This drops words that occur in more than half the documents.\n",
    "- min_df=2: This makes the vectorizer only use words that appear at least twice.\n",
    "- use_idf=True: This makes the vectorizer use inverse document frequencies in weighting.\n",
    "- norm=u'l2': This applies a correction factor so that longer and shorter documents get treated equally.\n",
    "- smooth_idf=True: This adds 1 to all document frequencies, as if an extra document existed that used every word once. This prevents divide-by-zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abominate</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abide  ability  able  abominate  abroad  absence  absent  absolute  \\\n",
       "0    0.0      0.0   0.0        0.0     0.0      0.0     0.0       0.0   \n",
       "1    0.0      0.0   0.0        0.0     0.0      0.0     0.0       0.0   \n",
       "2    0.0      0.0   0.0        0.0     0.0      0.0     0.0       0.0   \n",
       "3    0.0      0.0   0.0        0.0     0.0      0.0     0.0       0.0   \n",
       "4    0.0      0.0   0.0        0.0     0.0      0.0     0.0       0.0   \n",
       "\n",
       "   absolutely  absurd  ...  yes  yesterday  yield  you  young  youth  zeal  \\\n",
       "0         0.0     0.0  ...  0.0        0.0    0.0  0.0    0.0    0.0   0.0   \n",
       "1         0.0     0.0  ...  0.0        0.0    0.0  0.0    0.0    0.0   0.0   \n",
       "2         0.0     0.0  ...  0.0        0.0    0.0  0.0    0.0    0.0   0.0   \n",
       "3         0.0     0.0  ...  0.0        0.0    0.0  0.0    0.0    0.0   0.0   \n",
       "4         0.0     0.0  ...  0.0        0.0    0.0  0.0    0.0    0.0   0.0   \n",
       "\n",
       "   zealous                                               text   author  \n",
       "0      0.0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1      0.0  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2      0.0     remarkable Alice think way hear rabbit oh dear  Carroll  \n",
       "3      0.0                                            oh dear  Carroll  \n",
       "4      0.0                                         shall late  Carroll  \n",
       "\n",
       "[5 rows x 2889 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_tfidf = pd.concat([tfidf_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "\n",
    "# the log base 2 of 1 is 0,\n",
    "# so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "sentences_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.9048175182481751\n",
      "Test set score: 0.867338003502627\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9643795620437956\n",
      "Test set score: 0.8598949211908932\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.8464233576642336\n",
      "Test set score: 0.8235551663747811\n"
     ]
    }
   ],
   "source": [
    "# Scores on data with tf-idf features. \n",
    "model_and_evaluation(sentences_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF with Bigrams:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able bear</th>\n",
       "      <th>able persuade</th>\n",
       "      <th>absence home</th>\n",
       "      <th>absolute necessity</th>\n",
       "      <th>absolutely hopeless</th>\n",
       "      <th>accident lyme</th>\n",
       "      <th>accidentally hearing</th>\n",
       "      <th>accommodation man</th>\n",
       "      <th>account louisa</th>\n",
       "      <th>account small</th>\n",
       "      <th>...</th>\n",
       "      <th>young friend</th>\n",
       "      <th>young lady</th>\n",
       "      <th>young man</th>\n",
       "      <th>young people</th>\n",
       "      <th>young person</th>\n",
       "      <th>young sister</th>\n",
       "      <th>young woman</th>\n",
       "      <th>youth say</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able bear  able persuade  absence home  absolute necessity  \\\n",
       "0        0.0            0.0           0.0                 0.0   \n",
       "1        0.0            0.0           0.0                 0.0   \n",
       "2        0.0            0.0           0.0                 0.0   \n",
       "3        0.0            0.0           0.0                 0.0   \n",
       "4        0.0            0.0           0.0                 0.0   \n",
       "\n",
       "   absolutely hopeless  accident lyme  accidentally hearing  \\\n",
       "0                  0.0            0.0                   0.0   \n",
       "1                  0.0            0.0                   0.0   \n",
       "2                  0.0            0.0                   0.0   \n",
       "3                  0.0            0.0                   0.0   \n",
       "4                  0.0            0.0                   0.0   \n",
       "\n",
       "   accommodation man  account louisa  account small  ...  young friend  \\\n",
       "0                0.0             0.0            0.0  ...           0.0   \n",
       "1                0.0             0.0            0.0  ...           0.0   \n",
       "2                0.0             0.0            0.0  ...           0.0   \n",
       "3                0.0             0.0            0.0  ...           0.0   \n",
       "4                0.0             0.0            0.0  ...           0.0   \n",
       "\n",
       "   young lady  young man  young people  young person  young sister  \\\n",
       "0         0.0        0.0           0.0           0.0           0.0   \n",
       "1         0.0        0.0           0.0           0.0           0.0   \n",
       "2         0.0        0.0           0.0           0.0           0.0   \n",
       "3         0.0        0.0           0.0           0.0           0.0   \n",
       "4         0.0        0.0           0.0           0.0           0.0   \n",
       "\n",
       "   young woman  youth say                                               text  \\\n",
       "0          0.0        0.0  Alice begin tired sit sister bank have twice p...   \n",
       "1          0.0        0.0  consider mind hot day feel sleepy stupid pleas...   \n",
       "2          0.0        0.0     remarkable Alice think way hear rabbit oh dear   \n",
       "3          0.0        0.0                                            oh dear   \n",
       "4          0.0        0.0                                         shall late   \n",
       "\n",
       "    author  \n",
       "0  Carroll  \n",
       "1  Carroll  \n",
       "2  Carroll  \n",
       "3  Carroll  \n",
       "4  Carroll  \n",
       "\n",
       "[5 rows x 2619 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True, ngram_range=(2,2))\n",
    "\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_tf_bigram = pd.concat([tfidf_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "\n",
    "sentences_tf_bigram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.8160583941605839\n",
      "Test set score: 0.7683887915936952\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.8712408759124087\n",
      "Test set score: 0.8021015761821366\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.7640875912408759\n",
      "Test set score: 0.7548161120840631\n"
     ]
    }
   ],
   "source": [
    "# Scores on data with tf-idf and bigram features. \n",
    "model_and_evaluation(sentences_tf_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF with both 1 and 2-grams:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able bear</th>\n",
       "      <th>able persuade</th>\n",
       "      <th>abominate</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absence home</th>\n",
       "      <th>absent</th>\n",
       "      <th>...</th>\n",
       "      <th>young people</th>\n",
       "      <th>young person</th>\n",
       "      <th>young sister</th>\n",
       "      <th>young woman</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth say</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>remarkable Alice think way hear rabbit oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>shall late</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abide  ability  able  able bear  able persuade  abominate  abroad  absence  \\\n",
       "0    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "1    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "2    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "3    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "4    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "\n",
       "   absence home  absent  ...  young people  young person  young sister  \\\n",
       "0           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "1           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "2           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "3           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "4           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "\n",
       "   young woman  youth  youth say  zeal  zealous  \\\n",
       "0          0.0    0.0        0.0   0.0      0.0   \n",
       "1          0.0    0.0        0.0   0.0      0.0   \n",
       "2          0.0    0.0        0.0   0.0      0.0   \n",
       "3          0.0    0.0        0.0   0.0      0.0   \n",
       "4          0.0    0.0        0.0   0.0      0.0   \n",
       "\n",
       "                                                text   author  \n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2     remarkable Alice think way hear rabbit oh dear  Carroll  \n",
       "3                                            oh dear  Carroll  \n",
       "4                                         shall late  Carroll  \n",
       "\n",
       "[5 rows x 5506 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True, ngram_range=(1,2))\n",
    "\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences_tf_both = pd.concat([tfidf_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "\n",
    "sentences_tf_both.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.9094890510948905\n",
      "Test set score: 0.8629597197898424\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9635036496350365\n",
      "Test set score: 0.8633975481611208\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.8443795620437956\n",
      "Test set score: 0.824430823117338\n"
     ]
    }
   ],
   "source": [
    "# Scores on data with tf-idf and bigram features. \n",
    "model_and_evaluation(sentences_tf_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "It trains a shallow neural network model in an unsupervised manner for converting words to vectors. With a large enough corpus, this will eventually result in words that often appear together having vectors that are near one another.\n",
    "\n",
    "Parameters:\n",
    "- workers=4: We set the number of threads to run in parallel to 4 (make sense if your computer has available computing units).\n",
    "- min_count=1: We set the minimum word count threshold to 1.\n",
    "- window=6: We set the number of words around target word to consider to 6.\n",
    "- sg=0: We use CBOW because our corpus is small.\n",
    "- sample=1e-3: We penalize frequent words.\n",
    "- size=100: We set the word vector length to 100.\n",
    "- hs=1: We use hierarchical softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[remarkable, Alice, think, way, hear, rabbit, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[shall, late]</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  [Alice, begin, tired, sit, sister, bank, have,...  Carroll\n",
       "1  [consider, mind, hot, day, feel, sleepy, stupi...  Carroll\n",
       "2  [remarkable, Alice, think, way, hear, rabbit, ...  Carroll\n",
       "3                                         [oh, dear]  Carroll\n",
       "4                                      [shall, late]  Carroll"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify text preparation (to have sentences as a list of words in the final dataframe).\n",
    "\n",
    "# Clean text.\n",
    "# Remove title.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# Remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# Remove \"new line\" characters and other types of extra whitespaces.\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# Parse the cleaned novels.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "\n",
    "# Group into sentences.\n",
    "alice_sents = [[sent, 'Carroll'] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, 'Austen'] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "\n",
    "# Get rid of stop words and punctuation, lemmatize the tokens.\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec on the sentences.\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five words that are closer to lady:\n",
      "[('recommend', 0.9984135031700134), ('hand', 0.9983631372451782), ('constant', 0.9980081915855408), ('run', 0.9979991912841797), ('mind', 0.997941792011261)]\n",
      "The word that doesn't fit in list: dad dinner mom aunt uncle:\n",
      "dinner\n",
      "The similarity score of woman and man:\n",
      "0.99914813\n",
      "The similarity score of horse and cat:\n",
      "0.930524\n"
     ]
    }
   ],
   "source": [
    "# Explore word2vec representation.\n",
    "print('The first five words that are closer to lady:')\n",
    "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
    "print('The word that doesn\\'t fit in list: dad dinner mom aunt uncle:')\n",
    "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
    "print('The similarity score of woman and man:')\n",
    "print(model.similarity('woman', 'man'))\n",
    "print('The similarity score of horse and cat:')\n",
    "print(model.similarity('horse', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numerical features using word2vec representations of the words:\n",
    "- get the word2vec vectors of each word in a sentence and take the average of all the vectors in the high dimensional space (in this case it's 100). So, as a result, there is a vector of 100 dimensions as the feature for a sentence.\n",
    "- then use each dimension as a separate feature which means that in our the data set there will be 100 numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>0.071385</td>\n",
       "      <td>-0.291025</td>\n",
       "      <td>-0.049948</td>\n",
       "      <td>-0.108804</td>\n",
       "      <td>-0.231856</td>\n",
       "      <td>0.036387</td>\n",
       "      <td>-0.130457</td>\n",
       "      <td>-0.221261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229503</td>\n",
       "      <td>0.101283</td>\n",
       "      <td>-0.117701</td>\n",
       "      <td>0.545567</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>-0.150729</td>\n",
       "      <td>-0.103440</td>\n",
       "      <td>0.122948</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>-0.126771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>0.063728</td>\n",
       "      <td>-0.239920</td>\n",
       "      <td>-0.021090</td>\n",
       "      <td>-0.077444</td>\n",
       "      <td>-0.168184</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>-0.096978</td>\n",
       "      <td>-0.187447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182897</td>\n",
       "      <td>0.092881</td>\n",
       "      <td>-0.086390</td>\n",
       "      <td>0.430058</td>\n",
       "      <td>-0.007557</td>\n",
       "      <td>-0.136318</td>\n",
       "      <td>-0.072336</td>\n",
       "      <td>0.104083</td>\n",
       "      <td>0.030854</td>\n",
       "      <td>-0.100596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, rabbit, ...</td>\n",
       "      <td>0.077163</td>\n",
       "      <td>-0.314964</td>\n",
       "      <td>-0.050008</td>\n",
       "      <td>-0.108498</td>\n",
       "      <td>-0.247042</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>-0.133592</td>\n",
       "      <td>-0.243197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240807</td>\n",
       "      <td>0.116528</td>\n",
       "      <td>-0.120837</td>\n",
       "      <td>0.588547</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.179862</td>\n",
       "      <td>-0.098956</td>\n",
       "      <td>0.131845</td>\n",
       "      <td>0.042789</td>\n",
       "      <td>-0.142320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.077471</td>\n",
       "      <td>-0.248518</td>\n",
       "      <td>-0.007798</td>\n",
       "      <td>-0.060287</td>\n",
       "      <td>-0.187497</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>-0.127766</td>\n",
       "      <td>-0.226183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260143</td>\n",
       "      <td>0.125705</td>\n",
       "      <td>-0.103706</td>\n",
       "      <td>0.593541</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>-0.180068</td>\n",
       "      <td>-0.113480</td>\n",
       "      <td>0.108455</td>\n",
       "      <td>0.085983</td>\n",
       "      <td>-0.105960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[shall, late]</td>\n",
       "      <td>0.053746</td>\n",
       "      <td>-0.234428</td>\n",
       "      <td>-0.054857</td>\n",
       "      <td>-0.096483</td>\n",
       "      <td>-0.204009</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>-0.102644</td>\n",
       "      <td>-0.176613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176987</td>\n",
       "      <td>0.078345</td>\n",
       "      <td>-0.090648</td>\n",
       "      <td>0.418138</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>-0.111865</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>0.114342</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>-0.109687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,...  0.071385   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi...  0.063728   \n",
       "2  Carroll  [remarkable, Alice, think, way, hear, rabbit, ...  0.077163   \n",
       "3  Carroll                                         [oh, dear]  0.077471   \n",
       "4  Carroll                                      [shall, late]  0.053746   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0 -0.291025 -0.049948 -0.108804 -0.231856  0.036387 -0.130457 -0.221261  ...   \n",
       "1 -0.239920 -0.021090 -0.077444 -0.168184  0.027953 -0.096978 -0.187447  ...   \n",
       "2 -0.314964 -0.050008 -0.108498 -0.247042  0.050609 -0.133592 -0.243197  ...   \n",
       "3 -0.248518 -0.007798 -0.060287 -0.187497  0.004959 -0.127766 -0.226183  ...   \n",
       "4 -0.234428 -0.054857 -0.096483 -0.204009  0.040737 -0.102644 -0.176613  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0  0.229503  0.101283 -0.117701  0.545567 -0.000572 -0.150729 -0.103440   \n",
       "1  0.182897  0.092881 -0.086390  0.430058 -0.007557 -0.136318 -0.072336   \n",
       "2  0.240807  0.116528 -0.120837  0.588547  0.004665 -0.179862 -0.098956   \n",
       "3  0.260143  0.125705 -0.103706  0.593541 -0.009451 -0.180068 -0.113480   \n",
       "4  0.176987  0.078345 -0.090648  0.418138  0.006417 -0.111865 -0.076904   \n",
       "\n",
       "         97        98        99  \n",
       "0  0.122948  0.026550 -0.126771  \n",
       "1  0.104083  0.030854 -0.100596  \n",
       "2  0.131845  0.042789 -0.142320  \n",
       "3  0.108455  0.085983 -0.105960  \n",
       "4  0.114342  0.007152 -0.109687  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.7847946045370938\n",
      "Test set score: 0.8010110294117647\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9831391784181484\n",
      "Test set score: 0.8051470588235294\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.8991416309012875\n",
      "Test set score: 0.8350183823529411\n"
     ]
    }
   ],
   "source": [
    "model_and_evaluation(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use pre-trained vectors released by Google:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model_pretrained = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (4500, 302)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>-0.036288</td>\n",
       "      <td>0.082410</td>\n",
       "      <td>-0.010284</td>\n",
       "      <td>0.015515</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.035947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066516</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>-0.044208</td>\n",
       "      <td>-0.056998</td>\n",
       "      <td>-0.063269</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>-0.085071</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>-0.064371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>-0.002012</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>-0.066478</td>\n",
       "      <td>-0.035698</td>\n",
       "      <td>0.045293</td>\n",
       "      <td>-0.068695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055940</td>\n",
       "      <td>0.085838</td>\n",
       "      <td>-0.067052</td>\n",
       "      <td>-0.013628</td>\n",
       "      <td>-0.027802</td>\n",
       "      <td>-0.033665</td>\n",
       "      <td>-0.023586</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, rabbit, ...</td>\n",
       "      <td>0.072189</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>-0.009544</td>\n",
       "      <td>0.122665</td>\n",
       "      <td>-0.053543</td>\n",
       "      <td>-0.038696</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.055786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.050652</td>\n",
       "      <td>-0.086411</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.137391</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.061066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>-0.147949</td>\n",
       "      <td>0.099670</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>-0.093628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.094971</td>\n",
       "      <td>-0.052668</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>-0.142456</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.112671</td>\n",
       "      <td>-0.148193</td>\n",
       "      <td>0.186798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[shall, late]</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.206787</td>\n",
       "      <td>0.211182</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>-0.155762</td>\n",
       "      <td>0.088379</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021667</td>\n",
       "      <td>-0.103516</td>\n",
       "      <td>-0.038578</td>\n",
       "      <td>-0.007385</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.134155</td>\n",
       "      <td>-0.177246</td>\n",
       "      <td>-0.254639</td>\n",
       "      <td>-0.212158</td>\n",
       "      <td>0.087646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,...  0.046265   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi...  0.046331   \n",
       "2  Carroll  [remarkable, Alice, think, way, hear, rabbit, ...  0.072189   \n",
       "3  Carroll                                         [oh, dear]  0.073975   \n",
       "4  Carroll                                      [shall, late]  0.095215   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.016199 -0.036288  0.082410 -0.010284  0.015515  0.005437 -0.035947  ...   \n",
       "1  0.020463 -0.002012  0.101565 -0.066478 -0.035698  0.045293 -0.068695  ...   \n",
       "2  0.034546 -0.009544  0.122665 -0.053543 -0.038696  0.058594 -0.055786  ...   \n",
       "3  0.134277  0.141357  0.256348 -0.147949  0.099670  0.077148 -0.093628  ...   \n",
       "4  0.084473  0.206787  0.211182  0.043579 -0.155762  0.088379 -0.038574  ...   \n",
       "\n",
       "        290       291       292       293       294       295       296  \\\n",
       "0 -0.066516  0.029852 -0.042609 -0.044208 -0.056998 -0.063269  0.000244   \n",
       "1  0.055940  0.085838 -0.067052 -0.013628 -0.027802 -0.033665 -0.023586   \n",
       "2 -0.008102  0.050652 -0.086411  0.005266 -0.085938 -0.137391  0.004723   \n",
       "3  0.058228  0.000854 -0.094971 -0.052668 -0.091919 -0.142456 -0.053711   \n",
       "4 -0.021667 -0.103516 -0.038578 -0.007385  0.020264  0.134155 -0.177246   \n",
       "\n",
       "        297       298       299  \n",
       "0 -0.085071 -0.000340 -0.064371  \n",
       "1  0.009620  0.030316  0.000908  \n",
       "2  0.020203  0.000519  0.061066  \n",
       "3 -0.112671 -0.148193  0.186798  \n",
       "4 -0.254639 -0.212158  0.087646  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],300))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "  try:\n",
    "    word2vec_arr[i,:] = np.mean([model_pretrained[lemma] for lemma in sentence], axis=0)\n",
    "  except KeyError:\n",
    "    word2vec_arr[i,:] = np.full((1,300), np.nan)\n",
    "    continue\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "print(\"Shape of the dataset: {}\".format(sentences.shape))\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores:\n",
      "Training set score: 0.8892592592592593\n",
      "Test set score: 0.8594444444444445\n",
      "\n",
      "Random Forest Scores:\n",
      "Training set score: 0.9837037037037037\n",
      "Test set score: 0.7672222222222222\n",
      "\n",
      "Gradient Boosting Scores:\n",
      "Training set score: 0.9562962962962963\n",
      "Test set score: 0.8538888888888889\n"
     ]
    }
   ],
   "source": [
    "model_and_evaluation(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this assignment, I parsed text and explored data, reviewed most frequent tokens and lemmas, analyzed sentences and named entities. I then tried different language modeling methods, reviewed features and assessed them by training classification models (with default parameters) and evaluating scores and how well the models generalize on unseen data. Model performance varied based on input number and type of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
